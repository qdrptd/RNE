{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YTCrawl.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyON89tEMqXHo8nUb3avnu1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qdrptd/RNE/blob/main/YTCrawl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIoVy8pWIzKX"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "from openpyxl import Workbook\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "nGSpf29EI4hd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: UTF-8 -*-\n",
        "import time\n",
        "from selenium import webdriver\n",
        "import requests\n",
        "!pip install xlsxwriter\n",
        "#Colab에선 웹브라우저 창이 뜨지 않으므로 별도 설정한다.\n",
        " \n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')        # Head-less 설정\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver', options=options)"
      ],
      "metadata": {
        "id": "Asb1kLaNI7ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#해당 url로 이동\n",
        "pd_metadata = {}\n",
        "pd_comments = {}\n",
        "for i in range(1, 4):\n",
        "  type_n = open('type'+str(i)+\".txt\").readlines()\n",
        "  search_data = {}\n",
        "  for search in type_n:\n",
        "    # search = '메타버스'\n",
        "    url = \"https://www.youtube.com/results?search_query=\" + search #search 단어를 검색했을 때 나오는 페이지\n",
        "    driver.get(url)\n",
        "    driver.execute_script(\"window.scrollTo(0,1000)\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    # 페이지 끝까지 스크롤(복사)\n",
        "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    print(last_height)\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "        if new_height > 100000: #동영상은 꽤 많이 나와서, 페이지 길이에 한계를 설정\n",
        "          break\n",
        "\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    video_list = soup.select(\"a#video-title\")\n",
        "\n",
        "    id, titles = [], []\n",
        "    for video in video_list: \n",
        "      id.append(video['href']) #동영상 ID(링크) \n",
        "      titles.append(video['title']) #동영상 제목\n",
        "    views, channels, subs = [], [], []\n",
        "    channel_id_list = soup.select('div#channel-info > a')\n",
        "    views_list = soup.select(\"div#metadata > div > span:nth-of-type(1)\")\n",
        "    time_list = soup.select(\"div#metadata > div > span:nth-of-type(2)\")\n",
        "    for channel_id in channel_id_list:\n",
        "      channels.append(channel_id['href'])\n",
        "      channel_url = \"https://www.youtube.com\" + channel_id['href']\n",
        "      channel_html = requests.get(channel_url).text\n",
        "      channel_soup = BeautifulSoup(channel_html, 'html.parser')\n",
        "      subs.append(channel_soup.find('#subscriber-count'))\n",
        "    for v in views_list:\n",
        "      views.append(v.text)\n",
        "    search_data[search] = {\"아이디\" : id, \"제목\" : titles, \"조회수\": views, \"채널\": channels, \"구독자\": subs}\n",
        "    id_final = []\n",
        "    comment_final = []\n",
        "    video_comments_data = {}\n",
        "    for video_id in id:\n",
        "      url = \"https://www.youtube.com\" + video_id\n",
        "      try:\n",
        "        driver.get(url)\n",
        "      except:\n",
        "        break\n",
        "      driver.implicitly_wait(3)\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      driver.execute_script(\"window.scrollTo(0,1000)\")\n",
        "      time.sleep(3)\n",
        "\n",
        "      # 페이지 끝까지 스크롤\n",
        "      last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "\n",
        "      while True:\n",
        "          driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "          time.sleep(1.5)\n",
        "\n",
        "          new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "          if new_height == last_height:\n",
        "              break\n",
        "          last_height = new_height\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      # 팝업 닫기\n",
        "      try:\n",
        "          driver.find_element_by_css_selector(\"#dismiss-button > a\").click()\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "      # 대댓글 모두 열기\n",
        "      buttons = driver.find_elements_by_css_selector(\"#more-replies > a\")\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      for button in buttons:\n",
        "          button.send_keys(Keys.ENTER)\n",
        "          time.sleep(1.5)\n",
        "          try:\n",
        "            button.click()\n",
        "          except:\n",
        "            pass\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      # 정보 추출하기\n",
        "      html_source = driver.page_source\n",
        "      soup = BeautifulSoup(html_source, 'html.parser')\n",
        "\n",
        "      id_list = soup.select(\"div#header-author > h3 > #author-text > span\")\n",
        "      comment_list = soup.select(\"yt-formatted-string#content-text\")\n",
        "\n",
        "      \n",
        "\n",
        "      for i in range(len(comment_list)):\n",
        "        temp_id = id_list[i].text\n",
        "        temp_id = temp_id.replace('\\n', '')\n",
        "        temp_id = temp_id.replace('\\t', '')\n",
        "        temp_id = temp_id.replace('    ', '')\n",
        "        id_final.append(temp_id)\n",
        "\n",
        "        temp_comment = comment_list[i].text\n",
        "        temp_comment = temp_comment.replace('\\n', '')\n",
        "        temp_comment = temp_comment.replace('\\t', '')\n",
        "        temp_comment = temp_comment.replace('    ', '')\n",
        "        comment_final.append(temp_comment)\n",
        "      video_comments_data[video_id] = pd.DataFrame({'작성자': id_final, '내용': comment_final})\n",
        "      print(\"종료\")\n",
        "  pd_comments['type' + str(i)] = search_data\n",
        "\n",
        "pd_comments = pd.DataFrame(pd_comments)\n",
        "pd_comments.to_excel('comments.xlsx')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_17qaL4qJIB_",
        "outputId": "c28e91b0-c5c4-4009-dcd5-d58e88d3df6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "종료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "gcjqBdGnJqMG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}