{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qdrptd/RNE/blob/main/YTCrawl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIoVy8pWIzKX",
        "outputId": "7843d87e-cf3e-4b56-cb2c-3e050bb55718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
            "\u001b[K     |████████████████████████████████| 358 kB 39.4 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.5.18.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.2.0)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.2.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [813 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [982 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,022 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,293 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,517 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,830 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,015 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,262 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,036 kB]\n",
            "Get:23 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:24 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [47.7 kB]\n",
            "Fetched 16.2 MB in 5s (3,219 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 89 not upgraded.\n",
            "Need to get 89.8 MB of archives.\n",
            "After this operation, 302 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
            "Fetched 89.8 MB in 6s (16.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGSpf29EI4hd"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "from openpyxl import Workbook\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asb1kLaNI7ns",
        "outputId": "f02eb9fa-5860-446c-ef52-71741aec9c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 10.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.3\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: UTF-8 -*-\n",
        "import time\n",
        "from selenium import webdriver\n",
        "import requests\n",
        "!pip install xlsxwriter\n",
        "#Colab에선 웹브라우저 창이 뜨지 않으므로 별도 설정한다.\n",
        " \n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')        # Head-less 설정\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_17qaL4qJIB_",
        "outputId": "2cfa8eab-4dee-41b4-f5ed-ab66dca513b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['metaverse copyright problem\\n', 'metaverse sex offense\\n', 'metaverse bullying\\n', 'money laundering in metaverse\\n', '메타버스 코인 투기\\n']\n",
            "metaverse bullying\n",
            "\n",
            "4447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: DeprecationWarning: find_elements_by_css_selector is deprecated. Please use find_elements(by=By.CSS_SELECTOR, value=css_selector) instead\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "종료\n",
            "       타입                   검색어                    영상                 작성자  \\\n",
            "0       1  metaverse bullying\\n  /watch?v=sUHugSKBryw         Damon Katos   \n",
            "1       1  metaverse bullying\\n  /watch?v=sUHugSKBryw           DRENS2131   \n",
            "2       1  metaverse bullying\\n  /watch?v=sUHugSKBryw        Jake Simpson   \n",
            "3       1  metaverse bullying\\n  /watch?v=sUHugSKBryw            DhiyaNet   \n",
            "4       1  metaverse bullying\\n  /watch?v=sUHugSKBryw            LOVEIT38   \n",
            "...    ..                   ...                   ...                 ...   \n",
            "20366   1  metaverse bullying\\n  /watch?v=pU_9XPYwzFw    Jovanka Stamnova   \n",
            "20367   1  metaverse bullying\\n  /watch?v=pU_9XPYwzFw        Tanner Jones   \n",
            "20368   1  metaverse bullying\\n  /watch?v=pU_9XPYwzFw        Antonius Kok   \n",
            "20369   1  metaverse bullying\\n  /watch?v=pU_9XPYwzFw       Aleesa Blight   \n",
            "20370   1  metaverse bullying\\n  /watch?v=pU_9XPYwzFw      Nadine Huggins   \n",
            "\n",
            "                                                      내용            좋아요  \n",
            "0      I wasn’t even considering meta, now I’m ready ...  \\n    186\\n    \n",
            "1                                                          \\n    13\\n    \n",
            "2                                                      I    \\n    7\\n    \n",
            "3                                                 letsgo    \\n    5\\n    \n",
            "4                                                           \\n    2\\n    \n",
            "...                                                  ...            ...  \n",
            "20366                                        girls squad    \\n    0\\n    \n",
            "20367                                               3654    \\n    4\\n    \n",
            "20368                                        He’s trying    \\n    0\\n    \n",
            "20369                 I love your videos so much Rebecca    \\n    0\\n    \n",
            "20370   Jessy through Rebecca's I know ITIT's gonna work    \\n    3\\n    \n",
            "\n",
            "[350487 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "#해당 url로 이동\n",
        "pd_metadata = {}\n",
        "pd_comments = pd.DataFrame({})\n",
        "for t in range(1, 4):\n",
        "  type_n = open('type'+str(t)+\".txt\").readlines()\n",
        "  print(type_n)\n",
        "  search_data = {}\n",
        "  for search in type_n[2:]:\n",
        "    print(search)\n",
        "    driver = webdriver.Chrome('chromedriver', options=options)\n",
        "    url = \"https://www.youtube.com/results?search_query=\" + search #search 단어를 검색했을 때 나오는 페이지\n",
        "    driver.get(url)\n",
        "    driver.execute_script(\"window.scrollTo(0,1000)\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    # 페이지 끝까지 스크롤(복사)\n",
        "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    print(last_height)\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "        if new_height > 10000: #동영상은 꽤 많이 나와서, 페이지 길이에 한계를 설정\n",
        "          break\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    video_list = soup.select(\"a#video-title\")\n",
        "\n",
        "    id, titles = [], []\n",
        "    for video in video_list: \n",
        "      id.append(video['href']) #동영상 ID(링크)\n",
        "      titles.append(video['title']) #동영상 제목\n",
        "    views, channels, subs = [], [], []\n",
        "    channel_id_list = soup.select('div#channel-info > a')\n",
        "    views_list = soup.select(\"div#metadata > div > span:nth-of-type(1)\")\n",
        "    time_list = soup.select(\"div#metadata > div > span:nth-of-type(2)\")\n",
        "\n",
        "    for channel_id in channel_id_list:\n",
        "      try:\n",
        "        channels.append(channel_id['href'])\n",
        "        channel_url = \"https://www.youtube.com\" + channel_id['href']\n",
        "        channel_html = requests.get(channel_url).text\n",
        "        channel_soup = BeautifulSoup(channel_html, 'html.parser')\n",
        "        subs.append(channel_soup.select('#subscriber-count').text)\n",
        "      except:\n",
        "        pass\n",
        "    \n",
        "    for v in views_list:\n",
        "      views.append(v.text)\n",
        "    # search_data[search] = {\"아이디\" : id, \"제목\" : titles, \"조회수\": views, \"채널\": channels, \"구독자\": subs}\n",
        "    id_final = []\n",
        "    comment_final = []\n",
        "    likes_final = []\n",
        "    video_comments_data = {}\n",
        "    for vid_idx in range(len(id)):\n",
        "      video_id = id[vid_idx]\n",
        "      url = \"https://www.youtube.com\" + video_id\n",
        "      try:\n",
        "        driver.get(url)\n",
        "      except:\n",
        "        break\n",
        "      driver.implicitly_wait(3)\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      driver.execute_script(\"window.scrollTo(0,1000)\")\n",
        "      time.sleep(3)\n",
        "\n",
        "      # 페이지 끝까지 스크롤\n",
        "      last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "\n",
        "      while True:\n",
        "          driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "          time.sleep(1.5)\n",
        "\n",
        "          new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "          if new_height == last_height:\n",
        "              break\n",
        "          last_height = new_height\n",
        "          if new_height > 100000:\n",
        "            break\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      # 팝업 닫기\n",
        "      try:\n",
        "          driver.find_element_by_css_selector(\"#dismiss-button > a\").click()\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "      # 대댓글 모두 열기\n",
        "      buttons = driver.find_elements_by_css_selector(\"#more-replies > a\")\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      for button in buttons:\n",
        "          try:\n",
        "              button.send_keys(Keys.ENTER)\n",
        "          except:\n",
        "            pass\n",
        "          time.sleep(1.5)\n",
        "          try:\n",
        "            button.click()\n",
        "          except:\n",
        "            pass\n",
        "\n",
        "      time.sleep(1.5)\n",
        "\n",
        "      # 정보 추출하기\n",
        "      html_source = driver.page_source\n",
        "      soup = BeautifulSoup(html_source, 'html.parser')\n",
        "\n",
        "      id_list = soup.select(\"div#header-author > h3 > #author-text > span\")\n",
        "      comment_list = soup.select(\"yt-formatted-string#content-text\")\n",
        "      likes_list = soup.select('#vote-count-middle')\n",
        "      for i in range(len(comment_list)):\n",
        "        temp_id = id_list[i].text\n",
        "        temp_id = temp_id.replace('\\n', '')\n",
        "        temp_id = temp_id.replace('\\t', '')\n",
        "        temp_id = temp_id.replace('    ', '')\n",
        "        id_final.append(temp_id)\n",
        "\n",
        "        temp_comment = comment_list[i].text\n",
        "        temp_comment = temp_comment.replace('\\n', '')\n",
        "        temp_comment = temp_comment.replace('\\t', '')\n",
        "        temp_comment = temp_comment.replace('    ', '')\n",
        "        comment_final.append(temp_comment)\n",
        "\n",
        "        likes_final.append(likes_list[i].text)\n",
        "      pd_comments = pd.concat([pd_comments, pd.DataFrame({'타입': [t for temp in range(len(id_final))], '검색어': [search for temp in range(len(id_final))],'영상': [video_id for temp in range(len(id_final))], \\\n",
        "                                                          # '구독자': [subs[vid_idx] for temp in range(len(id_final))],\\\n",
        "                                                          '작성자': id_final, '내용': comment_final\\\n",
        "                                                          ,'좋아요': likes_final\n",
        "                                                          })])\n",
        "      print(\"종료\")\n",
        "    break\n",
        "  break\n",
        "  # pd_metadata['type' + str(i)] = search_data\n",
        "print(pd_comments)\n",
        "pd.DataFrame(pd_comments).to_excel(\"comments.xlsx\")\n",
        "# pd_metadata = pd.DataFrame(pd_metadata)\n",
        "# pd_metadata.to_excel('metadata.xlsx')\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GUG0wbQZG2n_"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(pd_comments).to_excel(\"comments.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmQjkLfyarvi",
        "outputId": "8fa7dfdf-fe65-4e3b-d876-c71340a4243a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "channel_url = \"https://www.youtube.com/cnaconnect\"\n",
        "driver = webdriver.Chrome('chromedriver', options=options)\n",
        "driver.get(channel_url)\n",
        "driver.execute_script(\"window.scrollTo(0,1000)\")\n",
        "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "\n",
        "while True:\n",
        "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    if new_height == last_height:\n",
        "        break\n",
        "    last_height = new_height\n",
        "    if new_height > 100000:\n",
        "        break\n",
        "try:\n",
        "    driver.find_element_by_css_selector(\"#dismiss-button > a\").click()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "channel_html = requests.get(channel_url).text\n",
        "channel_soup = BeautifulSoup(channel_html, 'html.parser')\n",
        "print(channel_soup.select('yt-formatted-string#subscriber-count'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcjqBdGnJqMG"
      },
      "outputs": [],
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4nIR_bklYkt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "YTCrawl.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXPTbOGL08aRZMJ40ou9bU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}